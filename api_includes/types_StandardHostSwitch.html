<!--
## ***************************************************************************
## Copyright 2022 VMware, Inc.  All rights reserved. VMware Confidential.
## ***************************************************************************
-->
<HTML>
<HEAD>
    <STYLE type="text/css">
      html {
        margin: 0px;
        padding: 0px;
      }

      body {
        background-color:#ffffff;
        color:#353833;
        font-family:Arial, Helvetica, sans-serif;
        font-size:14px;
      }
      #content {
        margin: 2ex 5ex 2ex 5ex;
        width: 100ex;
        font-family: verdana, aria, helvetica,sans-serif;
        font-size: 12px;
      }
      h1, h2, h3, h4 {
         font-family:'open sans', Arial, Helvetica, sans-serif;
      }

      h1, h2 {
        -webkit-margin-before: 0.5em;
        -webkit-margin-after: 0.5em;
      }

      h1 {
        font-size:2.5em;
        font-weight: 300;
      }

      h2 {
        font-size:2em;
        font-weight: 300;
      }

      h3 {
        font-size:1.5em;
        font-weight: 300;
      }

      h4 {
        font-weight: bold;
      }

      p {
        padding: 0;
        border: 0;
        margin: 0;
        padding-bottom: 1ex;
      }
      table {
        border-collapse: collapse;
        border: 1px solid #cccccc;
      }

      table td {
          border: 1px solid #cccccc;
          padding: 7px 5px 7px 5px;
          vertical-align: top;
          font-family:Arial, Helvetica, sans-serif;
          font-size:12px;
      }

      table th {
        color:#FFFFFF;
        background:#4a7db1;
        border: 1px solid #cccccc;
        padding: 5px;
        vertical-align: top;
        text-align: left;
        font-weight:bold;
      }

      span.schema_toggle, span.inline_table_toggle {
        border:1px solid #999999;
        margin-left:5px;
        color:#666666;
        cursor:pointer;
        padding:0px 2px;
      }
      span.expanded_schema {
        border:1px solid #CCCCCC;
        margin:4px 0px 0px 75px;
        display: block;
      }
      span.doc_category {
        margin-top: 20px;
        margin-bottom: 20px;
        font-size: 12px;
        display:block;
      }
      span.help_detail {
        display:block;
        margin:1ex 2ex 2ex 1ex;
      }
      span.example_request {
        display:block;
        margin:1ex 2ex 2ex 1ex;
        font-family: monospace;
        white-space: pre;
      }
      span.example_response {
        display:block;
        margin:1ex 2ex 2ex 1ex;
        font-family: monospace;
        white-space: pre;
      }
      span.rbac_feature {
        display:block;
        margin:1ex 2ex 2ex 1ex;
      }
      span.required_permissions {
        display:block;
        margin:1ex 2ex 2ex 1ex;
      }
      ul {
        margin-top: 5px;
        margin-bottom: 20px;
      }
      ol {
        counter-reset: section;                /* Creates a new instance of the section counter with each ol element */
        list-style-type: none;                 /* Nested couter creates outlined list in API table of contents */
      }
      ol li::before {
        counter-increment: section;            /* Increments only this instance of the section counter */
        content: counters(section,".") " ";    /* Adds the value of all instances of the section counter separated by a ".". */
      }
      .subsection_title {
        text-decoration: underline;
      }
      .subsection {
        margin: 5px 0px 0px 3em;
        width: 90%;
        background-color: #e8e8e8;
      }
      .subsection_text {
        padding: 3px;
        width: 100%;
        background-color: #f8f8f8;
      }
      .inline_table_div {
        clear: both;
        padding: 5px 0px;
        width: 100%;
        background-color: #ffffff;
      }
      .inline_table {
        margin-bottom: 0px
      }
      .error_table {
        margin: 7px 0 7px 5em;
        width: 90%;
      }
      tr.header {
        background-color: #cccccc;
      }
      .row {
        clear: both;
        width: 100%;
      }
      .label {
        padding: 3px;
        float: left;
        width: 25%;
      }
      .value {
        padding: 3px;
        float: right;
        width: 72%;
        background-color: #f8f8f8;
      }
      .api_object_title {
        font-weight: bold;
      }
      .experimental {
        padding-left: 5px;
        font-weight: normal;
        font-size: 12px;
        color: red;
      }
      .deprecated {
        padding-left: 5px;
        font-weight: normal;
        font-size: 12px;
        color: red;
      }
      .removed {
        padding-left: 5px;
        font-weight: normal;
        font-size: 12px;
        color: red;
      }
      .code {
        font-weight: normal;
        font-size: 12px;
        font-family: Lucida-Console, Monaco, monospace;
      }
      .code_block {
        font-weight: normal;
        font-size: 12px;
        font-family: Lucida-Console, Monaco, monospace;
        padding: 5px;
        padding-left: 15px;
      }
    </STYLE>
</HEAD>







<BODY>
    <h3>
      StandardHostSwitch (<a href="schemas_StandardHostSwitch.html">schema</a>)
        <span class="deprecated">(Deprecated)</span>
    </h3>
    <p>
    <span class="api_object_title">Standard host switch specification</span>
    <p>
    <span class="help_detail"></span>
    
    
    
    
    <table border="1">
      <tbody>
        <tr class="header">
          <th>Name</th>
          <th width="60%">Description</th>
          <th>Type</th>
          <th>Notes</th>
        </tr>
          <tr>
            <td>cpu_config</td>
            
            <td>Enhanced Networking Stack enabled HostSwitch CPU configuration<br><br>CPU configuration specifies number of Logical cpu cores (Lcores) per Non Uniform Memory Access (NUMA) node dedicated to Enhanced Networking Stack enabled HostSwitch to get the best performance.</td>
            
            <td>array of <a href="types_CpuCoreConfigForEnhancedNetworkingStackSwitch.html">CpuCoreConfigForEnhancedNetworkingStackSwitch</a></td>
            
            <td></td>
          </tr>
          <tr>
            <td>host_switch_id</td>
            
            <td>The host switch id. This ID will be used to reference a host switch.<br><br>This field is writable only in case of VDS type HostSwitch and system generated for NVDS type. For VDS type host switch, This field is used to lookup a VDS from corresponding Compute Manager and then can be configured for logical networking. For NVDS type host switch, This field is system generated and if provided will be overwritten.</td>
            
            <td>string</td>
            
            <td></td>
          </tr>
          <tr>
            <td>host_switch_mode</td>
            
            <td>Operational mode of a HostSwitch.<br><br>STANDARD - This mode applies to all the hypervisors. ENS - This is the Enhanced Data Path switch mode. This mode provides accelerated networking performances but also introduces additional prerequisites. In order to benefit from this mode, workloads will be need to be compiled with DPDK and will use VMXNET3 for their vNIC. This mode is only available on ESX hypervisor (6.7 and above, recommended 6.7 U2 and above) and unavailable on KVM, EDGE and Public Cloud Gateway. Not all NSX features are available in this mode, please consult the documentation. ENS_INTERRUPT - This is an interrupt driven variant of the Enhanced Data Path mode. Please, consult your account representative for applicability. This mode is available only on ESX hypervisor (7.0 and above).
<br>
If this property is specified, transport_zone_endpoints must be specified at StandardHostSwitch level.</td>
            
            <td>string</td>
            
            <td>Enum: STANDARD, ENS, ENS_INTERRUPT</td>
          </tr>
          <tr>
            <td>host_switch_name</td>
            
            <td>host switch name. This name will be used to reference a host switch.<br><br>This field is writable only in case of NVDS type HostSwitch and system generated for VDS type. For NVDS type host switch, If this name is unset or empty then the default host switch name will be used. The name must be unique among all host switches specified in a given transport node; unset name, empty name and the default host switch name are considered the same in terms of uniqueness. For VDS type host switch, Manager fetches VDS name from corresponding Compute Manager and populates this field. If VDS name is given (correct or incorrect) it is ignored and overwritten with correct one.</td>
            
            <td>string</td>
            
            <td><span style="color: rgb(153, 0, 0);">Deprecated</span><br>Default: "nsxDefaultHostSwitch"</td>
          </tr>
          <tr>
            <td>host_switch_profile_ids</td>
            
            <td>Identifiers of host switch profiles to be associated with this host switch.<br><br>Host switch profiles bound to this host switch. If a profile ID is not provided for any HostSwitchProfileType that is supported by the transport node, the corresponding default profile will be bound to the host switch. If transport node is created using Policy APIs, use policyPaths instead of UUIDs.</td>
            
            <td>array of <a href="types_HostSwitchProfileTypeIdEntry.html">HostSwitchProfileTypeIdEntry</a></td>
            
            <td></td>
          </tr>
          <tr>
            <td>host_switch_type</td>
            
            <td>Type of HostSwitch<br><br>VDS represents VMware vSphere Distributed Switch from vSphere that is used as HostSwitch through TransportNode or TransportNodeProfile configuration. When VDS is used as a HostSwitch, Hosts have to be added to VDS from vSphere and VDS instance is created on Hosts. To configure NSX on such hosts, you can use this VDS as a HostSwitch from NSX manager. vCenter has the ownership of MTU, LAG, NIOC and LLDP configuration of such VDS backed HostSwitch. Remaining configuration (e.g. UplinkHostswitchProfile) will be managed by NSX.
<br>
NVDS represents NSX Virtual Switch which is NSX native HostSwitch. All configurations of NVDS will be managed by NSX. HostSwitch of type NVDS has been deprecated on ESX hosts that are managed by a vCenter Server.</td>
            
            <td>string</td>
            
            <td>Enum: NVDS, VDS<br>Default: "NVDS"</td>
          </tr>
          <tr>
            <td>ip_assignment_spec</td>
            
            <td>Specification for IPs to be used with host switch virtual tunnel endpoints<br><br>IPs can come from either a static IP pool or an explicitly specified IP list or DHCP. In case a list of IP is specified, the number of IPs provided should be sufficient as per teaming policy associated with host switch uplink profile.</td>
            
            <td><a href="types_IpAssignmentSpec.html">IpAssignmentSpec</a><br>(Abstract type: pass one of the following concrete types)<br><a href="types_AssignedByDhcp.html">AssignedByDhcp</a><br><a href="types_StaticIpListSpec.html">StaticIpListSpec</a><br><a href="types_StaticIpMacListSpec.html">StaticIpMacListSpec</a><br><a href="types_StaticIpPoolSpec.html">StaticIpPoolSpec</a></td>
            
            <td></td>
          </tr>
          <tr>
            <td>is_migrate_pnics</td>
            
            <td>Migrate any pnics which are in use<br><br>When using the Quick Start workflow on 7.0 and above vCenter clusters, if the pnics specified in the pnics field are used by a single VSS HostSwitch, then they are migrated over to recommended VDS HostSwitch. If any two pnics are not used by the same VSS HostSwitch or VDS HostSwitch, it is not supported. In such cases, please migrate them in multiple steps, one VSS HostSwitch or VDS HostSwitch at a time.</td>
            
            <td>boolean</td>
            
            <td>Default: "False"</td>
          </tr>
          <tr>
            <td>pnics</td>
            
            <td>Physical NICs connected to the host switch<br><br>When using the Quick Start workflow on 7.0 and above vCenter clusters, pnic information will be populated by the recommendation engine when providing a VDS HostSwitch based recommendation for a VSS HostSwitch.</td>
            
            <td>array of <a href="types_Pnic.html">Pnic</a></td>
            
            <td></td>
          </tr>
          <tr>
            <td>pnics_uninstall_migration</td>
            
            <td>Physical NICs connected to a switch<br><br>This is only supported for NVDS type of host switch. If this is specified for VDS type of host switch, an error will be returned to user. The pnics to be migrated out to a non N-VDS switch during transport node deletion.</td>
            
            <td>array of <a href="types_Pnic.html">Pnic</a></td>
            
            <td><span style="color: rgb(153, 0, 0);">Deprecated</span></td>
          </tr>
          <tr>
            <td>portgroup_transport_zone_id</td>
            
            <td>Transport Zone ID representing the DVS used in NSX on DVPG<br><br>A transport zone will be created for each DVS found across all hosts in a cluster that is installed for NSX on DVPG. This field, populated by NSX, is the ID of the transport zone created for the DVS this host switch represents. All discovered segments created for the DVPGs found on the DVS will have this ID specified as the transport zone id.</td>
            
            <td>string</td>
            
            <td>Readonly</td>
          </tr>
          <tr>
            <td>transport_zone_endpoints</td>
            
            <td>Transport zone endpoints.<br><br>List of TransportZones that are to be associated with specified host switch.
<br>
If this property is specified, host_switch_mode at StandardHostSwitch level must be specified.</td>
            
            <td>array of <a href="types_TransportZoneEndPoint.html">TransportZoneEndPoint</a></td>
            
            <td></td>
          </tr>
          <tr>
            <td>uplinks</td>
            
            <td>Uplink/LAG of VMware vSphere Distributed Switch connected to the HostSwitch<br><br>If VDS is used as a HostSwitch this attribute must be specified. You can associate uplinks from UplinkHostSwitchProfile to either VDS uplink or LAG. VDS uplink or LAG will inherit the global VDS level teaming policy from vSphere. NSX managed uplink or LAG will have NSX teaming policy configured through UplinkHostSwitchProfile.</td>
            
            <td>array of <a href="types_VdsUplink.html">VdsUplink</a></td>
            
            <td></td>
          </tr>
          <tr>
            <td>vmk_install_migration</td>
            
            <td>The vmknic and logical switch mappings<br><br>When using the Quick Start workflow on 7.0 and above vCenter clusters, vmnk_install_migration will be populated by the recommendation engine when providing a VDS HostSwitch based recommendation for a VSS HostSwitch. It will contain The vmk interfaces and the associated logical switches on the HostSwitch. The state of this field is realized on the transport node during creation</td>
            
            <td>array of <a href="types_VmknicNetwork.html">VmknicNetwork</a></td>
            
            <td></td>
          </tr>
          <tr>
            <td>vmk_uninstall_migration</td>
            
            <td>The vmknic and portgroup mappings<br><br>This is only supported for NVDS type of host switch. If this is specified for VDS type of host switch, an error will be returned to user. The vmk interfaces and the associated portgroups on the VSS/DVS. This field is realized on the host during transport node deletion or NSX uninstallation to specify the destination for all vmks on N-VDS switches.</td>
            
            <td>array of <a href="types_VmknicNetwork.html">VmknicNetwork</a></td>
            
            <td><span style="color: rgb(153, 0, 0);">Deprecated</span></td>
          </tr>
      </tbody>
    </table>

    <br>
<!--
## ***************************************************************************
## Copyright 2022 VMware, Inc.  All rights reserved. VMware Confidential.
## ***************************************************************************
-->
</BODY>
</HTML>
